{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = 위에 코드창 생성, B = 아래에 코드창 생성\n",
    "Ctr+Enter = 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **웹 크롤링의 기초 이해**\n",
    "    - HTML 구조와 웹 페이지 요소를 이해하고, 이를 통해 데이터를 추출하는 방법을 배웁니다.\n",
    "- **파이썬 라이브러리 활용**\n",
    "    - Requests와 BeautifulSoup 같은 파이썬 라이브러리를 이용하여 웹 페이지에서 데이터를 효율적으로 수집하는 방법을 배웁니다.\n",
    "- **동적 웹 페이지 처리**\n",
    "    - Selenium을 사용하여 JavaScript로 동적으로 렌더링되는 웹 페이지에서 데이터를 추출하는 방법을 학습합니다.\n",
    "- **데이터 저장 및 분석**\n",
    "    - 수집한 데이터를 CSV나 JSON 파일로 저장하고, 기본적인 데이터 전처리 및 분석 방법을 배웁니다.\n",
    "- 크롤링 실제 활용\n",
    "    - 실제 프로젝트를 통해 뉴스 기사 데이터를 수집하고, 이를 분석하여 의미 있는 정보를 도출하는 과정을 경험합니다.\n",
    "\n",
    "### 웹 크롤링의 개념\n",
    "\n",
    "- **웹 크롤링(Web Crawling)**\n",
    "    - 자동화된 스크립트를 사용하여 인터넷 상의 웹 페이지를 방문하고 데이터를 수집하는 과정\n",
    "    - 웹 크롤링을 통해 웹 사이트의 텍스트, 이미지, 링크 등 다양한 정보를 수집\n",
    "- **크롤러(Crawler)**\n",
    "    - 웹 크롤링을 수행하는 프로그램 또는 스크립트\n",
    "    - 웹 페이지의 구조를 이해하고, 필요한 데이터를 추출하여 저장합니다.\n",
    "- **스크래핑(Scraping)**\n",
    "    - 특정 웹 페이지에서 필요한 데이터를 추출하는 과정\n",
    "    - 웹 크롤링과 스크래핑은 종종 혼용되어 사용되지만, 스크래핑은 크롤링의 일부분으로 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링의 활용 사례\n",
    "\n",
    "- **데이터 수집 및 분석:** 기업들은 웹 크롤링을 통해 경쟁사 분석, 시장 조사, 고객 리뷰 분석 등 다양한 목적으로 데이터를 수집하고 분석합니다.\n",
    "    - 예: e-커머스 사이트에서 상품 리뷰를 수집하여 분석\n",
    "- **뉴스 및 정보 수집:** 뉴스 기사, 블로그 포스트 등 최신 정보를 수집하여 트렌드 분석이나 여론 조사를 수행합니다.\n",
    "    - 예: 특정 주제에 대한 뉴스 기사를 수집하여 트렌드 분석\n",
    "- **학술 연구:** 연구자들은 웹 크롤링을 통해 대량의 데이터를 수집하여 연구에 활용합니다.\n",
    "    - 예: 소셜 미디어 데이터를 수집하여 사회적 현상 분석\n",
    "- **부동산 정보 수집:** 부동산 웹사이트에서 매물 정보를 수집하여 가격 비교 및 시장 분석에 활용합니다.\n",
    "    - 예: 부동산 매물 정보를 수집하여 지역별 가격 변동 분석\n",
    "- **주식 및 금융 데이터 수집:** 금융 웹사이트에서 주식 시세, 뉴스 등을 수집하여 투자 분석에 활용합니다.\n",
    "    - 예: 주식 시세 데이터를 수집하여 투자 전략 수립\n",
    "\n",
    "### 웹 크롤링의 장점 및 한계\n",
    "\n",
    "- **장점:**\n",
    "    - 대량의 데이터를 자동으로 수집할 수 있어 효율적\n",
    "    - 다양한 출처에서 데이터를 통합하여 분석 가능\n",
    "    - 사람의 개입 없이 24시간 데이터를 수집할 수 있음\n",
    "- **한계:**\n",
    "    - 웹사이트의 구조가 변경되면 크롤러가 작동하지 않을 수 있음\n",
    "    - 웹 크롤링은 법적, 윤리적 이슈를 동반할 수 있음\n",
    "    - 동적 웹 페이지의 경우, 크롤링이 어려울 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링 시 주의사항\n",
    "\n",
    "### 1. 법적 이슈\n",
    "\n",
    "- **로봇 배제 표준(Robots.txt):** 많은 웹사이트는 robots.txt 파일을 통해 크롤러가 접근할 수 있는 영역과 접근할 수 없는 영역을 지정합니다. 크롤러는 이 파일을 확인하고 규칙을 준수해야 합니다.\n",
    "    - 예: `User-agent: * Disallow: /private/`\n",
    "        \n",
    "        <aside>\n",
    "        💡 HTTP 통신의 헤더 부분입니다.\n",
    "        \n",
    "        </aside>\n",
    "        \n",
    "- **저작권:** 웹 크롤링을 통해 수집한 데이터가 저작권으로 보호될 수 있습니다. 특히, 상업적 목적으로 데이터를 사용할 경우 저작권 문제를 사전에 해결해야 합니다.\n",
    "- **서비스 약관:** 웹사이트의 서비스 약관(Terms of Service)에는 크롤링에 대한 규정이 포함될 수 있습니다. 약관을 위반할 경우 법적 문제가 발생할 수 있습니다.\n",
    "- **개인정보 보호:** 개인정보가 포함된 데이터를 수집할 경우 개인정보 보호법을 준수해야 합니다. 예를 들어, 사용자 데이터 수집 시 GDPR(유럽 일반 데이터 보호 규칙)이나 CCPA(캘리포니아 소비자 개인정보 보호법) 등을 준수해야 합니다.\n",
    "\n",
    "### 2. 윤리적 고려사항\n",
    "\n",
    "- **과도한 요청 피하기:** 웹 크롤링은 웹 서버에 부하를 줄 수 있습니다. 과도한 요청을 보내지 않도록 주의해야 하며, 적절한 딜레이를 설정하는 것이 좋습니다.\n",
    "    - 예: 각 요청 사이에 1-2초 간격 두기\n",
    "- **공정 사용:** 수집한 데이터를 사용할 때 공정 사용 원칙(Fair Use)을 준수해야 합니다. 이는 데이터 사용 목적, 성격, 양 등을 고려하여 공정하게 사용하는 것을 의미합니다.\n",
    "- **투명성:** 데이터를 수집할 때 가능한 한 투명하게 작업하고, 필요 시 데이터 소유자에게 알리는 것이 좋습니다.\n",
    "\n",
    "### 3. 기술적 주의사항\n",
    "\n",
    "- **웹사이트 구조 변경:** 웹사이트의 구조가 변경되면 크롤러가 작동하지 않을 수 있습니다. 이를 해결하기 위해 크롤러를 지속적으로 업데이트해야 합니다.\n",
    "- **IP 차단:** 빈번한 요청으로 인해 IP가 차단될 수 있습니다. 이를 피하기 위해 프록시 서버를 이용하거나 요청 빈도를 조절해야 합니다.\n",
    "- **CAPTCHA 회피:** 일부 웹사이트는 봇을 막기 위해 CAPTCHA를 사용합니다. 이를 우회하기 위해서는 CAPTCHA 솔버를 사용하거나 사람이 직접 해결해야 합니다.\n",
    "- **반복 가능성:** 크롤러는 여러 번 실행할 수 있어야 하며, 각 실행 결과가 일관되게 나와야 합니다. 이를 위해 코드를 잘 관리하고 테스트해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 정적 웹 페이지와 동적 웹 페이지\n",
    "\n",
    "**정적 웹 페이지 (Static Web Page)**\n",
    "\n",
    "- **정의**: 정적 웹 페이지는 서버에 저장된 HTML 파일을 클라이언트(웹 브라우저)에게 그대로 전달하는 웹 페이지입니다. 페이지의 내용은 변경되지 않으며, 모든 사용자가 동일한 콘텐츠를 보게 됩니다.\n",
    "- **특징**:\n",
    "    - 서버에서 HTML, CSS, JavaScript 파일을 클라이언트로 전달\n",
    "    - 사용자와의 상호작용이 제한적\n",
    "    - 서버 요청 시 동일한 콘텐츠 제공\n",
    "    - 예: 개인 포트폴리오 웹사이트, 간단한 정보 제공 사이트\n",
    "\n",
    "**동적 웹 페이지 (Dynamic Web Page)**\n",
    "\n",
    "- **정의**: 동적 웹 페이지는 서버 또는 클라이언트 측에서 생성된 데이터를 기반으로 페이지가 동적으로 구성되는 웹 페이지입니다. 사용자의 행동이나 데이터베이스의 변경에 따라 실시간으로 내용이 업데이트됩니다.\n",
    "- **특징**:\n",
    "    - 서버 측 스크립트(PHP, [ASP.NET](http://asp.net/) 등) 또는 클라이언트 측 스크립트(JavaScript)로 생성\n",
    "    - 사용자 입력에 따라 다른 콘텐츠 제공\n",
    "    - 데이터베이스와 상호작용 가능\n",
    "    - 예: 소셜 미디어, 전자상거래 사이트, 뉴스 포털\n",
    "\n",
    "### 동적 웹 페이지 처리가 왜 필요한가?\n",
    "\n",
    "**1. 동적 콘텐츠 제공:**\n",
    "\n",
    "- 동적 웹 페이지는 사용자 맞춤형 경험을 제공합니다. 예를 들어, 전자상거래 사이트는 사용자의 이전 구매 내역을 기반으로 제품을 추천합니다.\n",
    "\n",
    "**2. 실시간 데이터 업데이트:**\n",
    "\n",
    "- 동적 웹 페이지는 실시간으로 데이터를 업데이트하여 최신 정보를 사용자에게 제공합니다. 뉴스 사이트나 주식 거래 사이트가 이에 해당합니다.\n",
    "\n",
    "**3. 사용자 상호작용:**\n",
    "\n",
    "- 동적 웹 페이지는 사용자와의 상호작용을 통해 더 풍부한 사용자 경험을 제공합니다. 예를 들어, 댓글 시스템이나 실시간 채팅 기능이 있습니다.\n",
    "\n",
    "### 동적 웹 페이지 크롤링의 어려움과 처리 방법\n",
    "\n",
    "**왜 크롤링이 어려운가?**\n",
    "\n",
    "- **JavaScript로 렌더링**: 동적 웹 페이지는 서버에서 기본 HTML을 전송한 후, JavaScript가 실행되어 콘텐츠를 로드하거나 변경합니다. 단순한 HTML 파싱으로는 이러한 데이터를 얻을 수 없습니다.\n",
    "- **AJAX 요청**: 많은 동적 웹 페이지는 AJAX 요청을 통해 서버와 비동기적으로 데이터를 주고받습니다. 이러한 데이터는 초기 페이지 로드 시 존재하지 않으며, JavaScript가 실행된 후에야 나타납니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HTML 및 웹 페이지 구조 이해\n",
    "\n",
    "### 1. HTML 기본 구조\n",
    "\n",
    "- **HTML(Document Object Model):** HTML은 웹 페이지의 구조를 정의하는 마크업 언어입니다. HTML 문서는 요소로 구성되며, 각 요소는 태그로 감싸여 있습니다.\n",
    "    - 예: `<html>`, `<head>`, `<body>`, `<div>`, `<p>`, `<a>` 등\n",
    "- **기본 구조:** HTML 문서는 기본적으로 아래와 같은 구조를 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 주요 HTML 요소\n",
    "\n",
    "- **헤더 요소(head):** 문서의 메타데이터를 포함합니다. 예: `<title>`, `<meta>`, `<link>`, `<style>`, `<script>`\n",
    "- **본문 요소(body):** 웹 페이지의 주요 콘텐츠를 포함합니다. 예: `<header>`, `<nav>`, `<section>`, `<article>`, `<footer>`\n",
    "- **텍스트 요소:** 텍스트 콘텐츠를 정의합니다. 예: `<h1>`, `<p>`, `<span>`, `<strong>`, `<em>`\n",
    "- **링크 요소:** 다른 페이지로 연결하는 하이퍼링크를 정의합니다. 예: `<a href=\"URL\">Link Text</a>`\n",
    "- **이미지 요소:** 이미지를 포함합니다. 예: `<img src=\"image.jpg\" alt=\"Description\">`\n",
    "- **리스트 요소:** 목록을 정의합니다. 예: `<ul>`, `<ol>`, `<li>`\n",
    "\n",
    "### 3. 웹 페이지 구조 이해\n",
    "\n",
    "- **DOM 트리:** HTML 문서는 트리 구조로 표현됩니다. 각 HTML 요소는 노드로서 DOM(Document Object Model)을 구성합니다.\n",
    "    - 예: `<html>`은 루트 노드, `<body>`, `<head>`는 자식 노드\n",
    "- **속성(Attribute):** HTML 요소는 속성을 가질 수 있습니다. 예: `<a href=\"URL\" target=\"_blank\">Link Text</a>`\n",
    "- **클래스와 ID:** 스타일링과 스크립팅을 위해 HTML 요소에 클래스와 ID를 지정할 수 있습니다.\n",
    "    - 예: `<div class=\"container\">`, `<h1 id=\"main-title\">`\n",
    "\n",
    "### 4. CSS 선택자 이해\n",
    "\n",
    "- **CSS 선택자:** 웹 페이지에서 원하는 요소를 선택하고 스타일을 적용하는 방법을 이해하는 것이 중요합니다. 크롤링 시에도 요소를 선택할 때 사용됩니다.\n",
    "    - **기본 선택자:** `element`, `.class`, `#id`\n",
    "    - **복합 선택자:** `element.class`, `element#id`, `element > child`, `element + sibling`, `element ~ sibling`\n",
    "    - 예: `.container > .item`, `#main-title + p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 파이썬 크롤링 도구 소개\n",
    "\n",
    "웹 크롤링을 위해 파이썬에서 주로 사용되는 두 가지 주요 라이브러리는 Requests와 BeautifulSoup입니다. 이 두 라이브러리는 함께 사용되어 웹 페이지에서 데이터를 쉽게 수집할 수 있도록 도와줍니다.\n",
    "\n",
    "### 1. Requests 라이브러리\n",
    "\n",
    "- **소개:** Requests는 HTTP 요청을 보내고 응답을 받을 수 있게 해주는 파이썬 라이브러리입니다. 이를 통해 웹 페이지의 HTML 소스를 가져올 수 있습니다.\n",
    "- **설치:** `pip install requests`\n",
    "- **기본 사용법:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.naver.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTTP 응답 상태 코드 확인\n",
    "print(response.status_code)  # 200 (성공)\n",
    "\n",
    "# 응답 본문(HTML 소스) 확인\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BeautifulSoup 라이브러리\n",
    "\n",
    "- **소개:** BeautifulSoup은 HTML 및 XML 파일을 파싱하고 탐색할 수 있게 해주는 파이썬 라이브러리입니다. 이를 통해 원하는 데이터 요소를 쉽게 추출할 수 있습니다.\n",
    "- **설치:** `pip install beautifulsoup4`\n",
    "- **기본 사용법:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"<html><body><h1>Hello, World!</h1></body></html>\"\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 특정 요소 찾기\n",
    "heading = soup.find('h1').text\n",
    "print(heading)  # Hello, World!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Requests와 BeautifulSoup을 이용한 웹 크롤링\n",
    "\n",
    "- **단계 1:** Requests를 사용하여 웹 페이지의 HTML 소스를 가져옵니다.\n",
    "- **단계 2:** BeautifulSoup을 사용하여 HTML 소스를 파싱하고 원하는 데이터를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Sample HTML</title>\\n</head>\\n<body>\\n    <h1 id=\"main-title\">Welcome to the Sample Page</h1>\\n    <div class=\"content\">\\n        <p class=\"description\">This is a simple HTML page for Selenium testing.</p>\\n        <ul>\\n            <li class=\"item\">Item 1</li>\\n            <li class=\"item\">Item 2</li>\\n            <li class=\"item\">Item 3</li>\\n        </ul>\\n    </div>\\n    <footer>\\n        <p>Contact us at <a href=\"mailto:example@example.com\">example@example.com</a></p>\\n    </footer>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_code = \"\"\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample HTML</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 id=\"main-title\">Welcome to the Sample Page</h1>\n",
    "    <div class=\"content\">\n",
    "        <p class=\"description\">This is a simple HTML page for Selenium testing.</p>\n",
    "        <ul>\n",
    "            <li class=\"item\">Item 1</li>\n",
    "            <li class=\"item\">Item 2</li>\n",
    "            <li class=\"item\">Item 3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <footer>\n",
    "        <p>Contact us at <a href=\"mailto:example@example.com\">example@example.com</a></p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\"\"\n",
    "\n",
    "html_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n\\n\\n\\nSample HTML\\n\\n\\nWelcome to the Sample Page\\n\\nThis is a simple HTML page for Selenium testing.\\n\\nItem 1\\nItem 2\\nItem 3\\n\\n\\n\\nContact us at example@example.com\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BeautifulSoup으로 파싱\n",
    "soup = BeautifulSoup(html_code, 'html.parser')\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample HTML'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ID로 요소 찾기\n",
    "soup.find(\"title\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to the Sample Page'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id=\"main-title\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title This is a simple HTML page for testing.\n"
     ]
    }
   ],
   "source": [
    "# 2. 클래스 이름으로 요소 찾기\n",
    "soup.find(class_=\"title\")\n",
    "print(\"title\", description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1\n",
      "Item 2\n",
      "Item 3\n"
     ]
    }
   ],
   "source": [
    "# 2. find_all 이용.\n",
    "soup.find_all(class_=\"item\")\n",
    "\n",
    "for li in soup.find_all(class_= \"item\"):\n",
    "    print(li.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제: BeautifulSoup을 사용하여 다음을 수행하세요.\n",
    "\n",
    "1. ID가 `main-title`인 요소의 텍스트를 가져와 출력하세요.\n",
    "2. 클래스 이름이 `description`인 요소의 텍스트를 가져와 출력하세요.\n",
    "3. 태그 이름이 `li`인 모든 요소를 찾아 각각의 텍스트를 출력하세요.\n",
    "4. 링크 텍스트가 `example@example.com`인 요소의 `href` 속성을 가져와 출력하세요.\n",
    "5. CSS 선택자를 사용하여 ID가 `main-title`인 요소의 텍스트를 가져와 출력하세요.\n",
    "6. CSS 선택자를 사용하여 클래스 이름이 `description`인 요소의 텍스트를 가져와 출력하세요.\n",
    "7. CSS 선택자를 사용하여 `ul` 태그 안에 있는 클래스 이름이 `item`인 모든 `li` 요소를 찾아 각각의 텍스트를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to the Sample Page'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id=\"main-title\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to the Sample Page'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one(\"#main-title\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a simple HTML page for Selenium testing.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_=\"description\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a simple HTML page for Selenium testing.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one(\".description\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1\n",
      "Item 2\n",
      "Item 3\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all(class_=\"item\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1\n",
      "Item 2\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all(class_=\"item\", limit=2):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1\n",
      "Item 2\n"
     ]
    }
   ],
   "source": [
    "for item in soup.select(\".item\", limit=2):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item 1', 'Item 2', 'Item 3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트컴프레이션 이용.\n",
    "[item.text for item in soup.find_all(class_=\"item\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example@example.com'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(href=\"mailto:example@example.com\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example@example.com'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태그 사용.\n",
    "soup.find(\"a\", href=\"mailto:example@example.com\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 샘플 HTML\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample HTML</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 id=\"main-title\">Welcome to the Sample Page</h1>\n",
    "    <div class=\"content\">\n",
    "        <p class=\"description\">This is a simple HTML page for testing.</p>\n",
    "        <ul>\n",
    "            <li class=\"item\">Item 1</li>\n",
    "            <li class=\"item\">Item 2</li>\n",
    "            <li class=\"item\">Item 3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <footer>\n",
    "        <p>Contact us at <a href=\"mailto:example@example.com\">example@example.com</a></p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# BeautifulSoup으로 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 1. ID로 요소 찾기\n",
    "main_title = soup.find(id='main-title')\n",
    "print('Main Title:', main_title.text)\n",
    "\n",
    "# 2. 클래스 이름으로 요소 찾기\n",
    "description = soup.find(class_='description')\n",
    "print('Description:', description.text)\n",
    "\n",
    "# 3. 태그 이름으로 요소 찾기\n",
    "items = soup.find_all('li')\n",
    "for item in items:\n",
    "    print('List Item:', item.text)\n",
    "\n",
    "# 4. 링크 텍스트로 요소 찾기\n",
    "contact_link = soup.find('a', href='mailto:example@example.com')\n",
    "print('Contact Link:', contact_link['href'])\n",
    "\n",
    "# 5. CSS 선택자로 요소 찾기 (ID)\n",
    "main_title_css = soup.select_one('#main-title')\n",
    "print('Main Title (CSS):', main_title_css.text)\n",
    "\n",
    "# 6. CSS 선택자로 요소 찾기 (클래스 이름)\n",
    "description_css = soup.select_one('.description')\n",
    "print('Description (CSS):', description_css.text)\n",
    "\n",
    "# 7. CSS 선택자로 요소 찾기 (태그 안 클래스)\n",
    "items_css = soup.select('ul li.item')\n",
    "for item in items_css:\n",
    "    print('List Item (CSS):', item.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 웹 페이지에서 테이블 데이터 추출\n",
    "\n",
    "웹 페이지에서 테이블 데이터를 추출하는 것은 매우 일반적인 작업입니다. BeautifulSoup을 사용하여 HTML 테이블 데이터를 파싱하고 원하는 형식으로 추출하는 방법을 학습해보겠습니다.\n",
    "\n",
    "### 1. 테이블 데이터 추출 과정\n",
    "\n",
    "- **단계 1:** 웹 페이지의 HTML 소스를 Requests를 사용하여 가져옵니다.\n",
    "- **단계 2:** BeautifulSoup을 사용하여 HTML 소스를 파싱합니다.\n",
    "- **단계 3:** `find` 또는 `find_all` 메서드를 사용하여 테이블 요소를 찾습니다.\n",
    "- **단계 4:** 테이블 행(`<tr>`)과 셀(`<td>`, `<th>`)을 반복하면서 데이터를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 단계 1: 웹 페이지의 HTML 소스 가져오기\n",
    "url = \"<https://example.com/table-page>\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTTP 응답 상태 코드 확인\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "\n",
    "    # 단계 2: BeautifulSoup을 이용하여 HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # 단계 3: 테이블 요소 찾기\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # 단계 4: 테이블 행과 셀 반복하여 데이터 추출\n",
    "    headers = []\n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if row.find('th'):\n",
    "            headers = [cell.text.strip() for cell in cells]\n",
    "        else:\n",
    "            rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "    # 데이터프레임으로 변환 (옵션)\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 동적 웹 페이지 이해와 Selenium 사용법\n",
    "\n",
    "동적 웹 페이지는 클라이언트 측(JavaScript)에서 콘텐츠를 생성하거나 업데이트하는 웹 페이지를 의미합니다. 이런 페이지는 서버에서 HTML을 완전히 제공하지 않고, 웹 브라우저가 JavaScript 코드를 실행하여 콘텐츠를 로드하거나 업데이트합니다. 이러한 특성 때문에 정적 HTML 페이지와는 다르게 크롤링하는 방법이 필요합니다.\n",
    "\n",
    "### 1. 동적 웹 페이지의 특징\n",
    "\n",
    "- **JavaScript 실행:** 페이지 로드 시 JavaScript가 실행되어 콘텐츠가 동적으로 변경됩니다.\n",
    "- **Ajax 요청:** JavaScript는 서버에 추가 요청을 보내어 데이터를 가져와 페이지를 업데이트합니다.\n",
    "- **SPA(Single Page Application):** URL 변경 없이 페이지 내에서 동적으로 콘텐츠를 로드하여 사용자 경험을 향상시킵니다.\n",
    "\n",
    "### 2. 동적 웹 페이지 크롤링 방법\n",
    "\n",
    "동적 웹 페이지에서 데이터를 추출하려면 JavaScript를 실행할 수 있는 도구를 사용해야 합니다. 가장 일반적으로 사용되는 도구는 Selenium입니다.\n",
    "\n",
    "### 3. Selenium 사용법 소개\n",
    "\n",
    "Selenium은 웹 브라우저를 자동화하는 도구로, 웹 브라우저를 실제 사용자처럼 조작할 수 있습니다. 이를 통해 JavaScript가 렌더링한 콘텐츠를 크롤링할 수 있습니다.\n",
    "\n",
    "### 설치:\n",
    "\n",
    "- **Selenium 라이브러리:** `pip install selenium`\n",
    "- **웹 드라이버:** 크롤링하려는 브라우저에 맞는 웹 드라이버를 설치합니다. (예: ChromeDriver, GeckoDriver)\n",
    "\n",
    "### 기본 사용법:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# 브라우저 열기\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 웹 페이지 로드\n",
    "url = 'http://example.com'\n",
    "driver.get(url)\n",
    "\n",
    "# JavaScript 실행 대기\n",
    "time.sleep(3)  # 페이지 로드 및 JavaScript 실행을 위한 대기 시간\n",
    "\n",
    "# 요소 찾기\n",
    "heading = driver.find_element(By.ID, 'main-title').text\n",
    "print(\"Heading:\", heading)\n",
    "\n",
    "# 브라우저 닫기\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 크롤링한 데이터 저장 및 처리\n",
    "\n",
    "웹 크롤링을 통해 수집한 데이터를 유용하게 사용하기 위해서는 적절한 형식으로 저장하고, 필요에 따라 데이터를 전처리 및 분석하는 과정이 필요합니다. 이번 섹션에서는 CSV 및 JSON 파일로 데이터를 저장하는 방법과 간단한 데이터 전처리 및 분석 방법을 학습합니다.\n",
    "\n",
    "### 1. CSV 파일로 데이터 저장\n",
    "\n",
    "CSV(Comma-Separated Values) 파일은 데이터를 테이블 형태로 저장하는 간단한 텍스트 파일 형식입니다. 파이썬에서는 `pandas` 라이브러리를 사용하여 데이터를 CSV 파일로 쉽게 저장할 수 있습니다.\n",
    "\n",
    "### 설치:\n",
    "\n",
    "- `pandas` 라이브러리 설치: `pip install pandas`\n",
    "\n",
    "### 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 예시\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [30, 25, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('data.csv', index=False)\n",
    "print(\"Data saved to data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. JSON 파일로 데이터 저장\n",
    "\n",
    "JSON(JavaScript Object Notation) 파일은 데이터를 구조화된 형식으로 저장하는 파일 형식입니다. 파이썬에서는 `json` 모듈을 사용하여 데이터를 JSON 파일로 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 데이터 예시\n",
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 30, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 25, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 35, 'City': 'Chicago'}\n",
    "]\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "print(\"Data saved to data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리 및 간단한 분석\n",
    "\n",
    "수집한 데이터를 분석하기 전에 데이터 정리, 결측치 처리, 데이터 타입 변환 등의 전처리 과정을 거쳐야 합니다. 전처리 후에는 간단한 통계 분석을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일에서 데이터 로드\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())  # 결측치 처리\n",
    "df['City'] = df['City'].str.title()  # 문자열 정리\n",
    "\n",
    "# 데이터 분석: 나이의 평균, 최대, 최소 값 계산\n",
    "age_mean = df['Age'].mean()\n",
    "age_max = df['Age'].max()\n",
    "age_min = df['Age'].min()\n",
    "\n",
    "print(\"Age Mean:\", age_mean)\n",
    "print(\"Age Max:\", age_max)\n",
    "print(\"Age Min:\", age_min)\n",
    "\n",
    "# 결과를 새로운 CSV 파일로 저장\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "print(\"Processed data saved to processed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 심화 크롤링 및 프로젝트\n",
    "\n",
    "이번 섹션에서는 네이버 예약 사이트에서 예약 현황을 수집하는 실습을 진행해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- `selenium`: 웹 페이지의 자동화를 위한 도구입니다. 웹 브라우저를 열고 조작할 수 있습니다.\n",
    "- `webdriver`: Selenium의 웹 드라이버 모듈로, 웹 브라우저를 제어합니다.\n",
    "- `By`: Selenium의 모듈로, HTML 요소를 찾기 위한 다양한 방법을 제공합니다.\n",
    "- `BeautifulSoup`: HTML 및 XML 파일을 파싱하는 라이브러리입니다.\n",
    "\n",
    "이 코드에서는 Chrome 드라이버를 설정하고 있습니다. Chrome 드라이버는 Chrome 브라우저를 자동화하기 위해 사용됩니다. 다른 브라우저를 사용할 수도 있으며, 예를 들어 Firefox를 사용하려면 `webdriver.Firefox()`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 열기\n",
    "url = 'https://booking.naver.com/booking/12/bizes/181387/items/2870248'\n",
    "driver.get(url)\n",
    "# 페이지 로딩 대기 (필요 시)\n",
    "driver.implicitly_wait(10)  # 최대 10초 대기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- `driver.get(url)`: 지정된 URL을 엽니다.\n",
    "- `driver.implicitly_wait(10)`: 요소를 찾을 때까지 최대 10초까지 대기합니다. 요소가 즉시 나타나지 않더라도 대기하며, 나타나면 즉시 다음 작업을 수행합니다. 이 방법은 웹 페이지의 로딩 시간을 처리하는 데 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.24\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.25\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.26\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.27\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.28\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.29\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.30\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.31\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.32\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.33\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.34\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.35\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.36\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.37\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.38\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.39\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.40\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.41\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.42\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.43\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.44\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.45\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.46\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.47\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.48\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.49\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.50\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.51\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.52\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.53\")>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜 버튼들 찾기\n",
    "calendar_dates = driver.find_elements(By.CSS_SELECTOR, 'a.calendar-date')\n",
    "calendar_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- `driver.find_elements(By.CSS_SELECTOR, 'a.calendar-date')`: CSS 선택자를 사용하여 `a.calendar-date` 클래스가 있는 모든 요소를 찾습니다. `find_elements`는 여러 요소를 찾고 리스트를 반환합니다. 반대로 `find_element`는 첫 번째 요소만 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜별 예약 시간대 정보 저장할 리스트\n",
    "date_time_info = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- 예약 시간대 정보를 저장할 빈 리스트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"72128cea148f260ed88f9eff81ead7a7\", element=\"f.6F97CC50314761D825E4C6ED3D4DC1B1.d.C2E240CDD7EBDF0BE6C215C9474101A2.e.24\")>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_button = calendar_dates[18]\n",
    "date_button\n",
    "date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6일 11일, 오후 1, 1매\n",
      "6일 11일, 오후 5, 2매\n",
      "6일 12일, 오후 3, 1매\n",
      "6일 12일, 오후 5, 6매\n",
      "6일 13일, 오전 11, 3매\n",
      "6일 13일, 오후 3, 1매\n",
      "6일 13일, 오후 5, 3매\n",
      "6일 18일, 오전 11, 1매\n",
      "6일 19일, 오전 11, 5매\n",
      "6일 19일, 오후 1, 1매\n",
      "6일 19일, 오후 3, 2매\n",
      "6일 21일, 오전 11, 1매\n",
      "6일 25일, 오전 11, 4매\n",
      "6일 25일, 오후 1, 4매\n",
      "6일 25일, 오후 5, 1매\n",
      "6일 26일, 오전 11, 4매\n",
      "6일 26일, 오후 5, 2매\n",
      "6일 27일, 오후 1, 2매\n",
      "6일 27일, 오후 3, 2매\n",
      "6일 27일, 오후 5, 2매\n"
     ]
    }
   ],
   "source": [
    "for i, date_button in enumerate(calendar_dates):\n",
    "    try:\n",
    "        # 날짜 버튼 클릭\n",
    "        date_button.click()\n",
    "\n",
    "        # 페이지 로드 시간 대기 (필요 시 조정)\n",
    "        time.sleep(2)  # 페이지 로드 시간 대기\n",
    "        #driver.implicitly_wait(2)  # 페이지 로드 시간 대기\n",
    "        html_code = driver.page_source\n",
    "        soup = BeautifulSoup(html_code, \"html.parser\")\n",
    "        \n",
    "        list_times = soup.find_all(class_=\"list_time\")[0]\n",
    "        \n",
    "        for list_time in list_times:\n",
    "            list_time_text = list_time.find(class_=\"box_info_wrap\").text\n",
    "            sold_info = list_time_text[-2:]\n",
    "            if sold_info[-1] == \"매\":\n",
    "                day = i + 1\n",
    "                hour = list_time_text.split(\":\")[0]\n",
    "                print(f\"6일 {day}일, {hour}, {sold_info}\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- `for date_button in calendar_dates`: 각 날짜 버튼에 대해 반복합니다.\n",
    "- `date_button.click()`: 현재 날짜 버튼을 클릭합니다.\n",
    "- `time.sleep(2)`: 페이지가 로드될 시간을 주기 위해 2초간 대기합니다. `time.sleep`을 사용하는 대신 `driver.implicitly_wait`를 사용할 수도 있습니다.\n",
    "- `html = driver.page_source`: 현재 페이지의 HTML 소스를 가져옵니다.\n",
    "- `soup = BeautifulSoup(html, 'lxml')`: BeautifulSoup을 사용하여 HTML 소스를 파싱합니다. 여기서는 `lxml` 파서를 사용합니다. 다른 파서로는 `html.parser` 등이 있습니다.\n",
    "- `date_info = date_button.find_element(By.CSS_SELECTOR, 'span.num').text`: 날짜 버튼 내부의 날짜 정보를 추출합니다.\n",
    "- `timeslots = soup.select('div.time_box_wrap li.item')`: CSS 선택자를 사용하여 시간대 정보를 포함하는 모든 `li.item` 요소를 찾습니다.\n",
    "- 각 시간대에 대해 반복하며 시간과 예약 가능 여부를 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 파일에 저장\n",
    "import csv\n",
    "\n",
    "with open('date_time_info.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['date', 'time', 'availability']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for info in date_time_info:\n",
    "        writer.writerow(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명:\n",
    "\n",
    "- `csv` 모듈을 사용하여 결과를 CSV 파일로 저장합니다.\n",
    "- `with open('date_time_info.csv', 'w', newline='', encoding='utf-8') as csvfile`: CSV 파일을 쓰기 모드로 엽니다.\n",
    "- `fieldnames`: CSV 파일의 헤더를 정의합니다.\n",
    "- `csv.DictWriter`: 딕셔너리를 CSV 파일로 작성할 수 있는 객체를 생성합니다.\n",
    "- `writer.writeheader()`: 헤더를 작성합니다.\n",
    "- 각 정보를 CSV 파일에 작성합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
