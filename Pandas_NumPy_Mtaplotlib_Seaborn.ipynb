{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas, NumPy, Matplotlib, seaborn 이용한 데이터 분석\n",
    "- ctrl + shift + p -> reload window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 조작 및 분석을 위한 파이썬 라이브러리\n",
    "- 시리즈와 데이터프레임이라는 데이터분석에 유용한 데이터 구조 제공\n",
    "- 엘셀의 파이썬 버전\n",
    "- 대용량의 데이터를 처리하기 용이\n",
    "- 프로그래밍을 통한 반복적인 작업 및 자동화에 유리\n",
    "- 머신러닝/딥러닝 모델에 필요한 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "- pandas 라이브러리에서 지원하는 자료구조\n",
    "- 2차원의 행렬 데이터를 표 형태로 저장\n",
    "- 가로인 행(row)와 세로인 열(column)으로 이루어짐\n",
    "- 각 행과 열은 인덱스를 가지고 있어 데이터를 쉽게 검색하고 필터링 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series\n",
    "- pandas 라이브러리에서 지원하는 자료구조\n",
    "- 각 원소는 인덱스와 값으로 이루어짐\n",
    "- 인덱스는 숫자 또는 문자열로 지정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시리즈와 데이터프레임\n",
    "- 데이터프레임에서 하나의 열 데이터를 추출하면 시리즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치.\n",
    "%pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas, numpy를 불러온다.\n",
    "# pd, np라고 부른다고 약어 지정.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기: read_csv()\n",
    "- data폴더에 저장되어있는 데이터인 **seoul_park.csv** 파일을 불러와 데이터프레임 `df`에 저장.\n",
    "- csv 파일 형태를 가지므로 `read_csv()`를 활용하여 불러옵니다.\n",
    "- `read_csv()`에는 데이터가 저장되어있는 파일의 경로를 입력받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장 되어있는 데이터를 불러와서 데이터프레임으로 저장\n",
    "# .csv파일로 저장 되어있는 데이터는 read_csv() 를 사용\n",
    "# .xlsx등의 파일은 read_excel() 을 사용한다.\n",
    "df=pd.read_csv(\"./data/seoul_park.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구분자 : sep = '|' / '\\t'\n",
    "- 기본적으로 csv파일은 쉼표(comma)로 데이터 값이 구분되어 따로 구분자 설정 필요 없다.\n",
    "- 하지만 콤마(,)가 아닌 다른것으로 구분자가 설정되어 있을 경우.\n",
    "- 데이터를 그냥 불러오게 되면 에러가 발생.\n",
    "- 이러한 경우에 `sep`을 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('파일명', sep = '|')\n",
    "pd.read_csv('파일명', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코딩 : endcoding=\"utf-8\" / \"cp949\"\n",
    "- 불러오고자 하는 파일의 encoding과 python encoding의 설정이 맞지 않으면 에러 발생.\n",
    "- 이럴때 endcoding=\"utf-8\" / \"cp949\"로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('파일명', encoding='utf-8')\n",
    "pd.read_csv('파일명', encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 살펴보기: head(), tail()\n",
    "- 데이터프레임의 앞 뒤 일부 데이터를 확인하는데 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`head()`나 `tail()`메서드 안에 숫자를 입력하여 출력하는 데이터의 갯수를 지정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정보 확인: info(), type()\n",
    "\n",
    "- 데이터가 몇 개의 값을 가지는지, 어떤 자료형으로 저장되어있는지를 확인하는데에는 `info()`를 활용할 수 있습니다.\n",
    "- `type()`으로 타입만 확인 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df[\"날씨\"].isna(): 날씨 컬럼의 각 값이 NaN(결측값)인지 여부를 판단하여, 불리언 시리즈를 반환합니다. NaN인 경우는 True, 그렇지 않은 경우는 False입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"날씨\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df.info(verbose=False)는 일반적인 출력보다 간략하게 표시되도록 합니다.\n",
    "- 기본적으로 df.info()는 데이터프레임의 전체 요약 정보를 출력합니다.\n",
    "- 그러나 verbose=False 옵션을 사용하면 일부 정보를 생략하여 더 간단한 출력을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['날짜'], type(df['날짜'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컬럼 이름을 활용한 특정 컬럼 추출\n",
    "- 데이터프레임은 여러개의 시리즈가 열을 이루고 있는 자료형.\n",
    "- 대괄호를 사용하면 데이터프레임에서 **특정 시리즈(열)를 추출**할 수 있다.\n",
    "- 예를 들어 데이터프레임 `df`에서 \"청소년\" 열을 추출하는 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에서 컬럼이름을 활용해 특정 컬럼을 시리즈 형태로 추출가능.\n",
    "df['청소년']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 활용하여 여러 개의 컬럼을 데이터프레임 형태로 추출가능.\n",
    "df[['청소년','날씨']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 숫자 세기: value_counts()\n",
    "- `value_counts()`를 활용하면 컬럼에 어떤 값들이 몇 개씩 존재하는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"날씨\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 타입 변환의 필요성\n",
    "- 데이터는 항상 원하는 타입으로 되어있지 않다.\n",
    "- 실습파일은 숫자들이 정수 타입이 아니라 텍스트(object)로 되어 있다.\n",
    "- 텍스트 타입은 연산 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터프레임 변환 시 유의사항\n",
    "- pandas의 메서드는 원본 데이터프레임(시리즈)를 바로 변환하지 않고 변환된 데이터프레임(시리즈)를 반환\n",
    "- 원본 데이터프렝미을 변환하려면 덮어씌워주는 작업이 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 타입 변환: astype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터의 숫자들이 깔끔하게 정리되기는 했지만, `info`를 활용하여 살펴보면 여전히 데이터 타입은 텍스트(Object)인 것을 알 수 있습니다.\n",
    "- 통계적인 분석을 위해 해당 데이터들을 정수(int)형태로 바꿔주어야 합니다.\n",
    "\n",
    "- `astype()`을 사용하면 원하는 타입으로 변환할 수 있습니다.\n",
    "- `astype`메서드를 활용해 \"어른\" 컬럼의 데이터를 정수 형태로 변환하는 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"어른\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df`의 어른 컬럼의 데이터타입은 object로, `astype()`을 사용했지만 바뀌지 않은 것을 확인할 수 있습니다.\n",
    "\n",
    "- `astype()`을 비롯해 데이터프레임에 뭔가 변형을 가하거나 작업을 하는 메서드들은 데이터프레임 자체를 변환하지 않고, 변환된 새로운 데이터프레임을 반환합니다. \n",
    "\n",
    "- 따라서 `df[\"어른\"].astype(int)`이라는 코드는 `df`의 \"어른\" 컬럼을 정수형으로 변환하기는 하지만,\n",
    "- 그냥 정수형으로 바뀐 \"어른\" 컬럼을 시리즈 형태로 나타낼 뿐 **`df`의 \"어른\" 컬럼 그 자체가 바뀌는 것이 아닙니다**. \n",
    "\n",
    "- `df`의 \"어른\" 컬럼을 바꾸고 싶다면 `astype()`을 활용해 변환하여 생성한 \"어른\" 컬럼 시리즈를 `df`의 \"어른\" 컬럼에 덮어씌워주는 작업이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"어른\"]=df[\"어른\"].astype(int)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 타입 변환: to_numeric()\n",
    "\n",
    "- `to_numeric()`은 `astype()`과 같이 데이터의 형태를 변환하지만\n",
    "- 원하는 타입을 지정할 수 있는 `astype()`과 달리 숫자로만 변환한다는 차이점이 존재합니다.\n",
    "- 특히나 데이터분석에서는 데이터를 숫자로 변환할 일이 많기 때문에 유용하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"유료합계\"] = pd.to_numeric(df[\"유료합계\"])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreigner=df[\"외국인\"].astype(int)\n",
    "#foreigner=pd.to_numeric(df[\"외국인\"])\n",
    "print(foreigner.info()) # 주석을 해제하고 실행해 올바르게 형변환이 되어 저장됐는지 확인하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 for문을 사용해 입장객 수에 관련된 컬럼들을 모두 숫자형으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"유료합계\", \"어른\", \"청소년\", \"어린이\", \"단체\", \"무료합계\", \"총계\"]\n",
    "for column in columns:\n",
    "    df[column]=pd.to_numeric(df[column])\n",
    "    \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 타입 변환: to_datetime()\n",
    "\n",
    "- 날짜 컬럼에는 데이터가 수집된 날짜가 기록되어있습니다.\n",
    "- 하지만 위의 정보를 보면 알 수 있다시피 날짜는 텍스트로 저장되어있습니다.\n",
    "- 다시 말해 날짜 컬럼의 \"2016-01-01\"는 텍스트일 뿐 컴퓨터가 이것을 날짜로 인식할 수 없습니다.\n",
    "- 이 날짜를 보다 효과적으로 활용하기 위해 `to_datetime()`을 활용해 날짜컬럼 전체를 `datetime`형식으로 변환해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"날짜\"]= pd.to_datetime(df[\"날짜\"])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시간 타입은 날짜와 시간에 대한 정보가 담겨있어 다양한 처리가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['연']=df['날짜'].dt.year\n",
    "df['월']=df['날짜'].dt.month\n",
    "df['일']=df['날짜'].dt.day\n",
    "df['요일']=df['날짜'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 열 전체 변환: map()\n",
    "\n",
    "- 이 숫자들은 0부터 6까지, 월요일부터 일요일까지 숫자가 매칭되어있습니다.\n",
    "- 이 정보를 활용하여 \"요일\" 컬럼 전체 데이터를 변환하고자 `map()`을 사용합니다.\n",
    "\n",
    "- 먼저 `map()`에 변환 정보를 입력해주기 위해 숫자를 키로, 그에 대응하는 요일 글자를 값으로 가지는 `week` 딕셔너리를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df[\"컬럼이름\"].map(딕셔너리 등)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week={0:'월', 1:'화', 2:'수', 3:'목', 4:'금', 5:'토',6:'일'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 `map()`과 요일 정보가 담긴 `week` 딕셔너리를 사용해서 `df`의 \"요일\" 컬럼을 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['요일']=df['요일'].map(week)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"어른\", \"청소년\", \"어린이\"]]\n",
    "# 어른, 청소년, 어린이 컬럼만 추출.\n",
    "df[[\"어른\", \"청소년\", \"어린이\"]].map(lambda x: x * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터에 함수 적용: apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df[\"컬럼이름\"].apply(함수, axis = 0 or 1)`\n",
    "- `map`과 달리 복수의 컬럼에 사용 가능.\n",
    "- `axis`가 0이면 열 단위, 1이면 행 단위로 함수 적용(기본은 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['날씨'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 쓰는 방법\n",
    "def weather(e):\n",
    "    if e=='눈' or e=='비':\n",
    "        return '눈/비'\n",
    "    else:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df의 \"날씨\" 컬럼의 값들에 weather함수를 적용하여 변환한 뒤\n",
    "- df의 \"날씨\" 컬럼에 저장(덮어씌우기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 함수를 컬럼 전체에 적용하기 위해 `apply()`를 사용합니다. `apply()`는 특정 컬럼 혹은 데이터프레임 전체에 특정 함수를 적용할 때 사용 가능합니다. \n",
    "\n",
    "`df`의 \"날씨\" 컬럼에 `weather` 함수를 적용하려면 아래와 같이 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"날씨\"]=df[\"날씨\"].apply(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"날씨\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원하는 함수를 정의해서 `apply()`를 활용해 적용하는 과정은 굉장히 많이 사용.\n",
    "- 하지만 다양한 변환이 필요할수록 매번 함수를 만들고 사용해야하고, 이는 코드를 지저분하게 만듭니다.\n",
    "\n",
    "- 람다함수를 사용하면 위에서 `weather()` 함수를 정의하는 과정을 생략하고 다음과 같이 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 람다함수 쓰는 방법\n",
    "df[\"날씨\"]=df[\"날씨\"].apply(lambda e : \"눈/비\" if e==\"눈\" or e==\"비\" else e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값(NaN) 여부를 확인하고,\n",
    "# 각 값이 NaN이 아닌지 여부를 나타내는 불리언 데이터프레임을 반환\n",
    "df[[\"어른\", \"청소년\", \"어린이\"]].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"청소년\"].notna()][[\"어른\", \"청소년\", \"어린이\"]].apply(np.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어른 청소년 어린이의 평균값.\n",
    "df[[\"어른\", \"청소년\", \"어린이\"]].apply(np.average)\n",
    "df[[\"어른\", \"청소년\", \"어린이\"]].apply(lambda x: np.average(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 통계값 확인: mean(), min(), max(), median()\n",
    "\n",
    "- 데이터의 통계값을 확인하기 위해 각종 집계함수(`mean()`, `min()`, `max()`, `median()` 등)를 사용할 수 있습니다.\n",
    "- 예를들어 `mean()`을 활용해 \"어른\" 컬럼의 평균값을 구하려면 다음과 같이 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"어른\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 일차의 어른 입장객 수의 평균을 구할 수 있습니다.\n",
    "- 이번엔 총 입장객 수(\"총계\")가 가장 적은 날의 입장객 수를 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"총계\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전체 통계: describe()\n",
    "\n",
    "- 때로는 전체 통계값을 보고 이를 통해 데이터의 분석 방향을 결정하기도 합니다.\n",
    "- 이를 위해 데이터프레임의 다양한 통계값을 정리해서 보여주는 `describe()`를 사용합니다.\n",
    "\n",
    "- `describe()`는 숫자형 데이터들로 이루어진 컬럼들의 데이터의 갯수(count), 평균(mean), 표준편차(std), 최소값(min), 사분위수(25/50/75%), 최대값(max)를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 그룹화: groupby()\n",
    "\n",
    "- 데이터를 그룹으로 묶어서 확인할 수 있는 `groupby()`입니다. - `groupby()`를 사용하면 특정 기준에 따라 데이터를 정리해서 분석할 수 있습니다. \n",
    "- 우리는 `groupby()`를 사용해서 \"날씨\"를 기준으로 \"총계\"값의 평균을 구할 수 있습니다. \n",
    "- `groupby()`를 적용한 결과는 `DataFrame` 형태가 아닌 `DataFrameGroupBy` 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 컬럼을 지정하지 않고 전체 컬럼에 대한 집계도 가능\n",
    "- 2개 이상의 커럼을 기준으로 집계도 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby를 활용하여 \"요일\", \"공휴일\", 컬럼으로 데이터를 묶고\n",
    "# \"어른\" 입장객 수 최대값을 확인.\n",
    "# df.groupby([\"요일\",\"공휴일\"])[\"어른\"].max()\n",
    "# -> 데이터프레임처럼 보이지만 시리즈 이다.\n",
    "series = df.groupby([\"요일\",\"공휴일\"])[\"어른\"].max()\n",
    "# 시리즈 타입\n",
    "series = df.groupby([\"요일\",\"공휴일\"])[\"어른\"].max().reset_index()\n",
    "# 컬럼이 여러개 있는 데이터프레임 타입."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.loc[\"목\"][\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"날씨\")[\"총계\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"날씨\",\"공휴일\"])[\"총계\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"날씨\",\"공휴일\"])[[\"어른\", \"어린이\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas 논리연산자\n",
    "- `and -> & : (A > 30) & (A < 50)`\n",
    "- A가 30보다 크고 50보다 작다.\n",
    "- `or -> | :(A > 30) | (B < 50)`\n",
    "- A가 30보다 크거나 B가 50보다 작다.\n",
    "- (30 < A < 50) -> (안됨) 부등호는 한 번에 하나의 비교만 가능.\n",
    "- not -> ~\n",
    "- 소괄호 () 를 사용하여 우선 순위 명시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조건에 따른 인덱싱: Boolean indexing\n",
    "\n",
    "- 전체 데이터프레임에서 Boolean indexing을 통해 특정 조건에 맞는 데이터를 추출할 수 있습니다.\n",
    "- 예를 들어 어른 입장객 수가 5000보다 큰 날짜들의 데이터를 추출하려면 다음과 같이 코드를 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"어른\"]>5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas 논리연산자를 활용하면 좀 더 복잡하게 조건을 설정할 수 있습니다.\n",
    "- 두 개 이상의 조건을 and(`&`)나 or(`|`)을 활용하여 같이 사용하는데,\n",
    "- 이 때 각 조건식은 소괄호로 묶여있어야 합니다. \n",
    "\n",
    "- 어른 입장객의 수가 10000명이 넘는 공휴일의 데이터만을 추출하려면\n",
    "- and(`&`) 연산자를 활용해 다음과 같이 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"어른\"]>10000) & (df[\"공휴일\"]==\"O\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번에는 or(`|`)연산자를 활용해\n",
    "- 어른 입장객의 수가 10000명이 넘거나 어린이 입장객 수가 2000명이 넘는 날짜의 데이터를 추출해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"어른\"]>10000) | (df[\"어린이\"]>2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨을 활용한 데이터 추출: loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `loc`은 이러한 데이터 탐색 및 추출과정을 활용한 데이터 추출 메서드입니다.\n",
    "- **입력받은 데이터의 행과 열의 인덱스를 활용하여 그 위치에 해당하는 데이터를 추출**합니다. \n",
    "\n",
    "- 유의할 점은 우리가 사용하고있는 데이터의 행 인덱스는 날짜가 아니라 인덱스 숫자입니다.\n",
    "- 2016년 1월 4일 데이터의 인덱스값은 3이므로, `loc`을 사용하여 2016년 1월 4일의 어른 데이터를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3,'어른']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연속적인 객체(데이터프레임의 인덱스) 범위를 지정해 가져오는 방법인 슬라이싱을 활용하면\n",
    "- 범위를 지정하여 해당 범위에 해당하는 데이터들을 불러올 수도 있습니다.\n",
    "\n",
    "- `loc`은 라벨 기반 인덱싱을 사용하기 때문에\n",
    "- **`A:B`로 슬라이싱을 하면 A 부터 B까지, 즉 B포함한 범위를 인덱싱 한다**입니다.\n",
    "- 예를들어 `3:6` 의 범위를 지정한다면 인덱스가 3부터 6까지인 데이터, `\"어른\":\"외국인\"` 의 범위를 지정한다면\n",
    "- \"어른\"부터 \"외국인\" 까지의 데이터를 지정하게 됩니다. \n",
    "\n",
    "- 이렇게 슬라이싱을 활용해 추출한 데이터들은 복수의 데이터이므로,\n",
    "- 시리즈 혹은 데이터프레임 형태라는 사실을 알아두면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3:6,\"어른\":\"외국인\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loc과 Boolean indexing을 활용한 데이터 추출\n",
    "\n",
    "- Boolean indexing처럼 조건식과 논리연산자를 loc과 같이 활용하면 조건에 맞는 데이터들만을 추출할 수 있습니다.\n",
    "- 다음과 같이 어른과 어린이의 입장객 수가 둘다 1000보다 큰 날짜의 \"날짜\"부터 \"총계\" 행을 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['어른']>1000) & (df['어린이']>1000), \"날짜\":\"총계\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서를 활용한 데이터 추출: iloc\n",
    "\n",
    "- 행과 열의 정수 위치를 이용해 데이터를 추출하는 `iloc`\n",
    "- 앞서 `loc`이 행과 열의 이름을 좌표로 삼아 해당 위치의 데이터를 추출했다면,\n",
    "- `iloc`은 **행과 열의 정수형 위치, 즉 순서를 좌표로 삼아 해당 위치의 데이터를 추출**합니다.\n",
    "- 예를 들어 4번 행(2016년 1월 5일)의 7번 열(외국인) 데이터를 추출하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `iloc` 역시 슬라이싱을 활용하여 지정한 범위의 데이터를 추출할 수 있습니다. \n",
    "\n",
    "- `loc`과의 중요한 차이점이 있는데,\n",
    "- `iloc`은 위치 기반 인덱싱을 사용하여 범위를 지정하기 때문에 시작은 포함되고 끝은 포함되지 않는다.\n",
    "- 즉 `iloc`에서 **`A:B`로 슬라이싱을 하면 A부터 B-1까지, 즉 B를 포함하지 않는 범위를 인덱싱**합니다.\n",
    "- B를 포함해서 인덱싱하는 `loc`과는 다르기 때문에 코드를 작성하거나 해석할 때 유의하셔야 합니다.\n",
    "\n",
    "- `iloc`을 활용해서 인덱싱을 3&#126;6번 행, 4&#126;6번 열의 값을 추출하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3:7, 4:7] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `iloc`은 정수 위치를 사용하기 때문에 단순한 작업보다는, for문 등을 활용한 반복작업시에 매우 유용하게 활용할 수 있다.\n",
    "\n",
    "- `loc`과 `iloc`을 활용하면 특정 위치에 해당하는 데이터 값을 추출할 수도 있고,\n",
    "- 그 값을 다른 값으로 바꿔넣을 수도 있습니다. 예를 들어 2016년 1월 5일의 \"청소년\" 컬럼 값을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[4, \"청소년\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 이 값을 다른 값으로 바꿔주고 싶다면 `loc`이나 `iloc`을 활용해 값을 불러온 다음,\n",
    "- 바꿔줄 값을 `=`을 활용해 저장해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[4, \"청소년\"] = 100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loc VS iloc\n",
    "\n",
    "- loc은 index 이름을 활용, iloc은 정수형 위치(순서)를 활용하여 인덱싱\n",
    "- loc과 iloc을 활용하여 특정 원소 값 변환 가능\n",
    "- loc과 iloc을 활용해 바꿀 위치를 지정하고 해당 위치에 넣을 값을 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정제\n",
    "- 원본(Raw) 데이터는 불완전하거나 부정확한 경우가 많음\n",
    "- 데이터가 누락되어 있거나, 잘못된 값이 포함되어 있는 경우 등 다양한 문제가 존재\n",
    "- 데이터를 제거하거나 대체하는 등 다양한 정제과정이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정렬: sort_values\n",
    "- 데이터프레임의 특정 컬럼의 값을 기준으로 전체 데이터를 정렬할 때 사용\n",
    "- `sort_values()`는 특정 컬럼의 값을 기준으로 전체 데이터를 오름차순 혹은 내림차순으로 정렬합니다. \n",
    "- 입장객 수가 많은 순서를 알아보기 위해 \"총계\" 컬럼을 기준으로 내림차순으로 정렬해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ascending 이 True 이면 오름차순, False면 내림차순\n",
    "- (생략되어 있으면 기본값인 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 확인하기 위해 정렬한 데이터프레임을 df_sorted에 저장\n",
    "df_sorted=df.sort_values(\"총계\", ascending=False)\n",
    "\n",
    "# 입장객수가 많았던 날짜의 데이터 상위 10개 출력\n",
    "df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입장객 수 하위 10개 데이터\n",
    "df_sorted.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 재지정: reset_index\n",
    "\n",
    "- 다시 `df_sorted`를 살펴보시면 데이터는 \"총계\"컬럼에 맞춰서 정렬되었지만\n",
    "- 이 과정에서 기존의 인덱스를 그대로 가지고 정렬된 사실을 확인할 수 있습니다.\n",
    "- 이렇게 될 경우 앞서 배웠던 `loc` 등을 활용하기가 어렵기 때문에,\n",
    "- 이 데이터를 가지고 계속 데이터 분석을 할 예정이라면\n",
    "- 인덱스를 재정렬해줄 필요가 있습니다.\n",
    "- 이 때 우리는 `reset_index()`를 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.reset_index(drop=True) # drop을 True로 지정해주어 \"index\"컬럼 생성 안함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 삭제: drop\n",
    "\n",
    "- 데이터프레임에서 특정 행이나 열을 삭제할 때 사용\n",
    "- `df=df.drop([\"유료합계\", \"무료합계\"], axis=1)`\n",
    "- df에 저장(덮어씌우기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 2개의 컬럼을 리스트로 입력\n",
    "# axis=1로 설정하여 컬럼을 삭제\n",
    "df=df.drop([\"유료합계\", \"무료합계\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 열 이름 바꾸기: rename\n",
    "\n",
    "- 데이터프레임의 특정 열 이름을 바꾸는데 사용할 때 사용.\n",
    "- `df=df.rename(columns = {\"총계\": \"총입장객수\"})`\n",
    "- 변환 뒤 df에 저장(덮어씌우기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns = {\"총계\": \"총입장객수\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치 탐색: isnull()\n",
    "\n",
    "- 데이터프레임의 각 원소가 결측치인지 여부를 True/False로 반환\n",
    "- 먼저 결측치가 얼마나 존재하는지 알아보기 위해 `isnull()`을 활용합니다.\n",
    "- sum()과 함께 사용하여 각 컬럼별로 결측치가 몇 개 존재하는지 확인 하는 용도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isnull()`은 데이터프레임의 각 원소가 결측치인지 아닌지를 검사합니다. 결측치가 아닌 제대로된 값은 False, 결측치는 True로 채워진것을 확인할 수 있습니다. 하지만 저 표를 가지고는 결측치가 컬럼별로 몇개인지 확인할 수가 없습니다. 이 때 True는 1로, False는 0으로 계산되어 `sum()`을 활용해서 각 컬럼별로 전체값의 합을 구하면 결과값은 컬럼에 존재하는 True의 갯수, 즉 결측치의 갯수가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df.isnull` : 데이터프레임의 각 요소가 `NaN`인지 여부 확인하여 `True` / `False` 값을 반환하는 불리언 데이터프레임을 생성.\n",
    "- `df.isna()` : `isnull`과 동일.\n",
    "- `df.notna` : `df.isna()`의 반대 동작을 수행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결측치를 처리하는 방법은 대표적으로 2가지가 있습니다.\n",
    "- 하나는 결측치를 **특정 값으로 채워넣는 것**이고,\n",
    "- 하나는 **결측치가 존재하는 데이터를 삭제하는 것**입니다. \n",
    "\n",
    "## 결측치 채우기: fillna()\n",
    "\n",
    "- `fillna()`는 결측치를 특정 값으로 채워넣는데 활용하는 메서드로,\n",
    "- 어떤 값으로 채워넣는지는 다양한 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(int(df[\"청소년\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치 데이터 삭제하기: dropna()\n",
    "\n",
    "- 결측치가 많지 않을 경우, 결측치가 있는 데이터를 삭제해버리는 것도 좋은 방법입니다.\n",
    "- 가령 청소년 컬럼에서 결측치가 있는 데이터를 삭제하면 5일치의 데이터가 사라지는 것입니다.\n",
    "- 이는 전체 기간 1086일의 데이터에서 비중이 크지 않기 때문에 괜찮은 방법이 될 수도 있습니다.\n",
    "- 이럴 때 `dropna()`를 활용하면 알아서 기준 컬럼에서 결측치를 탐사하고 해당하는 데이터를 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=\"청소년\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(int(df[\"청소년\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 병합\n",
    "- 데이터가 여러 개로 분산되어 있거나, 추가 데이터를 합치고 싶을 경우\n",
    "- 데이터 병합을 통해 데이터의 일관성을 유지하고 분석 결과의 신뢰도를 높힘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 병합: concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(\"./data/seoul_park_april.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, df2], axis=0, join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concat()`을 활용하면 두 개 이상의 데이터프레임을 행 또는 열 방향으로 단순히 이어붙이는데 활용합니다. 지금의 경우 2019년 3월 31일까지의 데이터인 `df`의 아래방향으로 2019년 4월의 데이터 `df2`를 이어붙이면 되기 때문에 `concat()`을 활용합니다.\n",
    "\n",
    "아래 방향으로 붙이기 때문에 `axis`를 **0**으로, 두 데이터프레임에 모두 존재하는 컬럼만을 남기기 위해 `join`을 **inner**로, 인덱스를 전체 초기화하기 위해 `ignore_index`를 **True**로 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 병합: merge()\n",
    "\n",
    "데이터 병합은 정보의 종류를 늘리는데도 활용할 수 있습니다. 서울대공원 데이터셋과 미세먼지 데이터셋을 합치면, 날씨 뿐 만 아니라 미세먼지 농도에 따른 입장객 수의 정보를 확인할 수도 있습니다. 서울대공원 데이터셋과 같은 기간동안 수집된 미세먼지 데이터를 불러오도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df, mm, on = '날짜', how = 'left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
